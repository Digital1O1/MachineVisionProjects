{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d42dd00",
   "metadata": {},
   "source": [
    "\n",
    "# Flower Species Classifier Project\n",
    "\n",
    "Welcome! This project is designed to help you solidify key concepts in deep learning, such as:\n",
    "- Building and training a Keras model.\n",
    "- Understanding activation functions (logits, softmax, sigmoid, etc.).\n",
    "- Using optimizers (especially ADAM) and understanding gradient descent.\n",
    "- Applying regularization techniques (L1, L2, dropout, and batch normalization).\n",
    "- Fine-tuning hyperparameters and employing early stopping.\n",
    "- Evaluating model performance with appropriate metrics.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c6036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88483eea",
   "metadata": {},
   "source": [
    "# `Sparse representation` versus `One hot encoding`\n",
    "\n",
    "---\n",
    "\n",
    "## What do they do\n",
    "- Represent cateogrical data numerically \n",
    "- Where they differ --> How they handle large numbers of categories \n",
    "\n",
    "## Sparse  representation \n",
    "- More memory efficient \n",
    "  - By storing only `non-zero` values \n",
    "  - Makes it great for situations where most of the values are 0\n",
    "\n",
    "## One hot encoding \n",
    "- Creates seperate binary column for `each category`\n",
    "- Doing so can lead to potentially large, sparse matricies when dealing with alot of categories\n",
    "\n",
    "---\n",
    "\n",
    "## Reasoning why one hot encoding was picked \n",
    "### Simplicity and interpretability\n",
    "  - One hot encoding super straight forward to implement/interpret \n",
    "  - Each class represented by a vector \n",
    "    - Elements represented as `1`\n",
    "    - The rest are represented as `0`\n",
    "    - Example for 3 classes \n",
    "      - Class 0: [1, 0, 0]\n",
    "      - Class 1: [0, 1, 0]\n",
    "      - Class 2: [0, 0, 1]\n",
    "\n",
    "### Compatibility with loss functions\n",
    "- Most classification loss functions like `cross entropy` \n",
    "  - `Expects` target labels to be `one-hot encoded` \n",
    "  - This format lets the loss function to compute probabilities for `each class` and `compares them with the actual class label`\n",
    "- If labels were `NOT` one-hot encoded\n",
    "  - Model would need extra logic to figure out which class corresponds to the ground truth\n",
    "\n",
    "## Works well with Softmax\n",
    "- Softmax function `used in the final layer` in of a multi-class classifier outputs the probabilities for each class \n",
    "- One hot encoded labels natrually align with these probabilities making the training process smooth\n",
    "\n",
    "## Avoids Implicit Ordinal Assumptions \n",
    "- Not like integer encoding that would use 0,1,2,3\n",
    "- One hot encoding ensure the model doesn't interpret the labels as ordinal (having meaningful order)\n",
    "  - This prevents the model from assuming relationships like \"Class 2 > Class 1 > Class 0.\"\n",
    "\n",
    "\n",
    "# When is One-Hot Encoding Chosen for Simplicity?\n",
    "## Small Number of Classes:\n",
    "- One-hot encoding is especially simple and computationally efficient when the number of classes is small. \n",
    "- For example, in the flower classification task, \n",
    "  - There might be only 5-10 classes, making one-hot encoding a natural choice.\n",
    "\n",
    "## Ease of Implementation:\n",
    "- For beginners, one-hot encoding is easier to understand and implement than alternative methods like label encoding or embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e457ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape :  (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Load the CIFAR-10 dataset (can be replaced with a flower dataset later)\n",
    "# # What is CIFAR-10 : colection of images used to train ML and CV algorithms\n",
    "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# # Normalize images to [0, 1] range\n",
    "# x_train = x_train.astype(\"float32\") / 255.0\n",
    "# x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# # One-hot encode labels\n",
    "# num_classes = 10\n",
    "# y_train = to_categorical(y_train, num_classes)\n",
    "# y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# # Class names for CIFAR-10 dataset\n",
    "# CLASS_NAMES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# #print(len(CLASS_NAMES))\n",
    "\n",
    "# # Visualize some sample images\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for i in range(len(CLASS_NAMES)):\n",
    "#     plt.subplot(2, 5, i + 1)\n",
    "#     plt.imshow(x_train[i])\n",
    "#     plt.title(CLASS_NAMES[np.argmax(y_train[i])])\n",
    "#     plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Load the dataset (e.g., CIFAR-10 or any flower dataset)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "num_classes = 10\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Prepare training and testing datasets\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(buffer_size=1024)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "input_shape = x_train.shape[1:]  # Get the input shape for the model\n",
    "\n",
    "print(\"Input shape : \", input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592839be",
   "metadata": {},
   "source": [
    "# Notes \n",
    "\n",
    "## MaxPooling2D(poolSize_)\n",
    "- Reference link : https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
    "- Syntax: keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", data_format=None, name=None, **kwargs)\n",
    "- Downsamples input along it's spatial dimensions (heigh and width)\n",
    "- Done by taking `maximum value` over an input window of `size defined by pool_size` for each channel of the input\n",
    "- The window mentioned is shifted by `strides` along each dimension \n",
    "- Resulting output \n",
    "  - When using the `valid` padding\n",
    "    - Has a spatial shape (number of rows x columns) of `output_shape = math.floor((input_shape - pool_size) / strides) + 1 `\n",
    "    - Only when `input_shape >= pool_size`\n",
    "  - When using the `same` padding option : `output_shape = math.floor((input_shape - 1) / strides) + 1`\n",
    "\n",
    "## BatchNormalization()\n",
    "- `What does it do`\n",
    "  - Normalizes the inputs to a layer across the mini batch to have :\n",
    "    - A mean of `0`\n",
    "    - A standard deviation of `1`\n",
    "  - `Speeds up training `\n",
    "    - When nomralizing the inputs, BN `reduces the amount` by which the internal network activations shift during training\n",
    "      - Aka : `internal covariate shift`\n",
    "    - Doing so `allows model to converge faster` and `reduce sensitivity to initial weights`\n",
    "  - `Reduces dependence on initialization `\n",
    "    - W/o BN\n",
    "      - Neural networks usually sensitive to the `initialization of weights` \n",
    "      - Poor initialization --> Slow training or failture to converge \n",
    "\n",
    "## Purpose of flattening a tensor\n",
    "- `Converts Multi-dimensional Data into a Linear Format`\n",
    "  - Ensures compatibility between `convolutional layer` to the `fully connected layers`\n",
    "    - Note \n",
    "    -   Convolutional layers : work with 2D/3D tensors \n",
    "    -   The fully connected layers expect a 1D input\n",
    "  -   As mentioned earlier, flattening ensures compatibility between the layers\n",
    "-   Preserves relevant features \n",
    "    -   Keeps spatial information extracted by previous layers such as\n",
    "        -   Convolutions \n",
    "        -   Pooling\n",
    "    -   Flattening converts it into a single vector \n",
    "-   Simplifies matrix operations \n",
    "    -   Dense layer work with 1D arrays are both simpler and computationally efficient when it comes to dealing with linear data structures and matrix multplication \n",
    "-   Enables integration with dense layers \n",
    "    -   Dense layers require the input vector format to compute the `dot product` between weights and inputs \n",
    "    -   Flattening provides this format\n",
    "-   Flattening ensures compatibility with other libraries \n",
    "    -   Makes transferring data to libraries/frameworks outside the neural networks alot easier \n",
    "    -   For cases like further data analysis or further processing\n",
    "-   Flattening supports various output shapes\n",
    "    -   Lets you adapt the output of convolutional layers; depending on kernel size/stride/input shapes, to a fixed-size intput for dense layers \n",
    "    -   Makes the model flexible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e492cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m295,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">316,106</span> (1.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m316,106\u001b[0m (1.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">315,914</span> (1.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m315,914\u001b[0m (1.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define a CNN model\n",
    "def build_model(input_shape, num_classes):\n",
    "    filters = 32\n",
    "    kernalSize = (3,3)\n",
    "    poolSize_ = (2,2) # Tuple --> Immutable collection of elements \n",
    "\n",
    "    model = Sequential([\n",
    "        # Conv2D(Filters,KernelSize,Strides,Padding,ActivationFunction)\n",
    "        # Conv2D(32, (3, 3), activation='relu', input_shape=input_shape)\n",
    "        Conv2D(filters,kernalSize, activation='relu', input_shape=input_shape),\n",
    "\n",
    "        # keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", data_format=None, name=None, **kwargs\n",
    "        #MaxPooling2D(((2,2))),\n",
    "        MaxPooling2D(((poolSize_))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "    input_shape = x_train.shape[1:]\n",
    "        - Extract shape of the input data, excluding the batch size\n",
    "        - shape[1:]\n",
    "            - Python slicing \n",
    "            - Extracts subset of a sequence like a tuple/list/array\n",
    "            - Takes all elements of the shape starting from the SECOND element since this indexes at 0\n",
    "                - This results in x_train.shape[1:] resulting in (32,32,3)\n",
    "        - Neural network input shapes \n",
    "            - When defining models in Keras the input shape tells the model the dimensions of the data for each sample\n",
    "        \n",
    "    Why is this important in NN\n",
    "        - Generalization across batches \n",
    "            - The model works with any batch size during training/inference \n",
    "            - By excludingg the batch size the input shape can adapt dynamically\n",
    "        - Defining layers\n",
    "            - Layers like Input() or Conv2D() needs the input shape to be known to process the data \n",
    "            - Providing the input shape ensure the model is built correctly\n",
    "                \n",
    " \n",
    "'''\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "model = build_model(input_shape, num_classes)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f79f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e39eee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Fit the data generator to training data\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digital101/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2025-01-20 21:33:23.611567: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[64,32,30,30]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,3,32,32]{3,2,1,0}, f32[32,3,3,3]{3,2,1,0}, f32[32]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2025-01-20 21:33:23.676301: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[64,64,13,13]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,32,15,15]{3,2,1,0}, f32[64,32,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 43ms/step - accuracy: 0.3003 - loss: 2.2945 - val_accuracy: 0.3894 - val_loss: 1.9877\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.4696 - loss: 1.4769 - val_accuracy: 0.5440 - val_loss: 1.2864\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 39ms/step - accuracy: 0.5104 - loss: 1.3771 - val_accuracy: 0.5454 - val_loss: 1.3219\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.5395 - loss: 1.2944 - val_accuracy: 0.5960 - val_loss: 1.1524\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - accuracy: 0.5590 - loss: 1.2433 - val_accuracy: 0.6267 - val_loss: 1.0557\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.5657 - loss: 1.2250 - val_accuracy: 0.5852 - val_loss: 1.1653\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.5784 - loss: 1.1953 - val_accuracy: 0.6486 - val_loss: 1.0128\n",
      "Epoch 8/10\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5879 - loss: 1.1609"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model with data augmentation\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=64),\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83827342",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#subplot(nrows, ncols, index, **kwargs)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(dropout_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mdropout_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropout Rate vs Test Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropout Rate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test_accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
